{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# A4 Classification of Hand-Drawn Digits\n",
    "\n",
    "In this assignment, you will define a new class named `NeuralNetworkClassifier` that extends the `NeuralNetwork` class provided here and is the solution to Assignment A2.  You will use `NeuralNetworkClassifier` to train a classifier of hand-drawn digits.\n",
    "\n",
    "You will also define the function `confusion_matrix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## `NeuralNetwork` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:37.969620Z",
     "start_time": "2023-04-17T20:18:37.809570Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following code cell will write its contents to `optimizers.py` so the `import optimizers` statement in the code cell after it will work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:38.081155Z",
     "start_time": "2023-04-17T20:18:37.817573Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting optimizers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimizers.py\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "## class Optimizers()\n",
    "######################################################################\n",
    "\n",
    "class Optimizers():\n",
    "\n",
    "    def __init__(self, all_weights):\n",
    "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
    "        \n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        # The following initializations are only used by adam.\n",
    "        # Only initializing m, v, beta1t and beta2t here allows multiple calls to adam to handle training\n",
    "        # with multiple subsets (batches) of training data.\n",
    "        self.mt = np.zeros_like(all_weights)\n",
    "        self.vt = np.zeros_like(all_weights)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.beta1t = 1\n",
    "        self.beta2t = 1\n",
    "\n",
    "        \n",
    "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= learning_rate * grad\n",
    "\n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
    "        epsilon = 1e-8\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
    "            self.beta1t *= self.beta1\n",
    "            self.beta2t *= self.beta2\n",
    "\n",
    "            m_hat = self.mt / (1 - self.beta1t)\n",
    "            v_hat = self.vt / (1 - self.beta2t)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ion()\n",
    "\n",
    "    def parabola(wmin):\n",
    "        return ((w - wmin) ** 2)[0]\n",
    "\n",
    "    def parabola_gradient(wmin):\n",
    "        return 2 * (w - wmin)\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "\n",
    "    wmin = 5\n",
    "    optimizer.sgd(parabola, parabola_gradient, [wmin],\n",
    "                  n_epochs=500, learning_rate=0.1)\n",
    "\n",
    "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "    optimizer.adam(parabola, parabola_gradient, [wmin],\n",
    "                   n_epochs=500, learning_rate=0.1)\n",
    "    \n",
    "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:38.099159Z",
     "start_time": "2023-04-17T20:18:37.844087Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimizers\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class NeuralNetwork()\n",
    "######################################################################\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
    "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
    "            self.n_hiddens_per_layer = []\n",
    "        else:\n",
    "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
    "\n",
    "        # Initialize weights, by first building list of all weight matrix shapes.\n",
    "        n_in = n_inputs\n",
    "        shapes = []\n",
    "        for nh in self.n_hiddens_per_layer:\n",
    "            shapes.append((n_in + 1, nh))\n",
    "            n_in = nh\n",
    "        shapes.append((n_in + 1, n_outputs))\n",
    "\n",
    "        # self.all_weights:  vector of all weights\n",
    "        # self.Ws: list of weight matrices by layer\n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Define arrays to hold gradient values.\n",
    "        # One array for each W array with same shape.\n",
    "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
    "\n",
    "        self.trained = False\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "        self.Tmeans = None\n",
    "        self.Tstds = None\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        # vector of all weights built by horizontally stacking flatenned matrices\n",
    "        # for each layer initialized with uniformly-distributed values.\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
    "        # into correct shape for each layer.\n",
    "        views = []\n",
    "        start = 0\n",
    "        for shape in shapes:\n",
    "            size =shape[0] * shape[1]\n",
    "            views.append(all_weights[start:start + size].reshape(shape))\n",
    "            start += size\n",
    "        return all_weights, views\n",
    "\n",
    "\n",
    "    # Return string that shows how the constructor was called\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
    "\n",
    "\n",
    "    # Return string that is more informative to the user about the state of this neural network.\n",
    "    def __str__(self):\n",
    "        result = self.__repr__()\n",
    "        if len(self.error_trace) > 0:\n",
    "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
    "\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        '''\n",
    "train: \n",
    "  X: n_samples x n_inputs matrix of input samples, one per row\n",
    "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
    "  n_epochs: number of passes to take through all samples updating weights each pass\n",
    "  learning_rate: factor controlling the step size of each update\n",
    "  method: is either 'sgd' or 'adam'\n",
    "        '''\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tmeans = T.mean(axis=0)\n",
    "            self.Tstds = T.std(axis=0)\n",
    "            \n",
    "        # Standardize X and T\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        T = (T - self.Tmeans) / self.Tstds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        if len(self.Tstds) == 1:\n",
    "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        else:\n",
    "            error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "\n",
    "    def relu(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s\n",
    "\n",
    "    def grad_relu(self, s):\n",
    "        return (s > 0).astype(int)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        '''X assumed already standardized. Output returned as standardized.'''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        return self.Ys\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
    "        return mean_sq_error\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.Ys[-1]\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X)\n",
    "        Y = Ys[-1]\n",
    "        # Unstandardize output Y before returning it\n",
    "        return Y * self.Tstds + self.Tmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:38.173685Z",
     "start_time": "2023-04-17T20:18:37.863090Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 25 Error=0.24970\n",
      "Adam: Epoch 50 Error=0.23331\n",
      "Adam: Epoch 75 Error=0.20884\n",
      "Adam: Epoch 100 Error=0.16481\n",
      "Adam: Epoch 125 Error=0.10720\n",
      "Adam: Epoch 150 Error=0.07256\n",
      "Adam: Epoch 175 Error=0.05813\n",
      "Adam: Epoch 200 Error=0.04995\n",
      "Adam: Epoch 225 Error=0.04358\n",
      "Adam: Epoch 250 Error=0.03816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cf7d525d60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(100).reshape((-1, 1))\n",
    "T = (X - 20) ** 3 / 300000\n",
    "\n",
    "hiddens = [10]\n",
    "nnet = NeuralNetwork(X.shape[1], hiddens, T.shape[1])\n",
    "nnet.train(X, T, 250, 0.01, method='adam')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nnet.error_trace)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(T, label='T')\n",
    "plt.plot(nnet.use(X), label='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Your `NeuralNetworkClassifier` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Complete the following definition of `NeuralNetworkClassifier` as discussed in class. You will need to override the functions\n",
    "\n",
    "* `train`\n",
    "* `error_f`\n",
    "* `gradient_f`\n",
    "* `use`\n",
    "\n",
    "and define the following new functions\n",
    "\n",
    "* `makeIndicatorVars`\n",
    "* `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:38.173685Z",
     "start_time": "2023-04-17T20:18:38.154680Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m<tokenize>:65\u001B[1;36m\u001B[0m\n\u001B[1;33m    def error_f(self, X, T):\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class NeuralNetworkClassifier:\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "        self.all_weights = []\n",
    "        self.Ws = []\n",
    "\n",
    "        if isinstance(n_hiddens_per_layer, int):\n",
    "            n_hiddens_per_layer = [n_hiddens_per_layer]\n",
    "\n",
    "        for i, n_hiddens in enumerate(n_hiddens_per_layer):\n",
    "            if i == 0:\n",
    "                n_in = n_inputs\n",
    "            else:\n",
    "                n_in = n_hiddens_per_layer[i-1]\n",
    "            self.all_weights.extend(np.random.uniform(-0.1, 0.1, size=(n_in + 1) * n_hiddens))\n",
    "            self.Ws.append((n_in + 1, n_hiddens))\n",
    "\n",
    "        self.all_weights.extend(np.random.uniform(-0.1, 0.1, size=(n_hiddens_per_layer[-1] + 1) * n_outputs))\n",
    "        self.Ws.append((n_hiddens_per_layer[-1] + 1, n_outputs))\n",
    "        self.all_weights = np.array(self.all_weights)\n",
    "\n",
    "    def unpack(self, weights):\n",
    "        first = 0\n",
    "        for (m, n), W in zip(self.Ws, self.Vs):\n",
    "            last = first + m * n\n",
    "            W[:] = weights[first:last].reshape((m, n))\n",
    "            first = last\n",
    "\n",
    "    def pack(self, Vs):\n",
    "        return np.hstack([V.flat for V in Vs])\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Zs = []\n",
    "        Zprev = X\n",
    "        for W in self.Vs[:-1]:\n",
    "            Zprev = np.tanh(np.dot(Zprev, W[1:, :]) + W[0:1, :])\n",
    "            self.Zs.append(Zprev)\n",
    "\n",
    "        Y = np.dot(Zprev, self.Vs[-1][1:, :]) + self.Vs[-1][0:1, :]\n",
    "        Y = self.softmax(Y)\n",
    "        return Y\n",
    "\n",
    "    def train(self, X, T, n_iterations, method='adam', verbose=True):\n",
    "        def error_f(weights, X, T):\n",
    "            self.unpack(weights)\n",
    "            Y = self.forward(X)\n",
    "            return -np.sum(np.log(Y) * T)\n",
    "\n",
    "        def gradient_f(weights, X, T):\n",
    "            self.unpack(weights)\n",
    "            Y = self.forward(X)\n",
    "            delta = -(T - Y) / (T.shape[0] * T.shape[1])\n",
    "            dVs = self.backpropagate(delta)\n",
    "            return self.pack(dVs)\n",
    "\n",
    "        self.Vs = [np.resize(W, (n_in + 1, n_out)) for (n_in, n_out), W in zip(self.Ws, self.Vs)]\n",
    "\n",
    "\n",
    "        all_weights, _ = optimizers.Optimizers(self.all_weights)\n",
    "        self.unpack(all_weights)\n",
    "\n",
    "    def error_f(self, X, T):\n",
    "       Y = self.forward(X)\n",
    "       return -np.mean(np.log(Y) * T)\n",
    "\n",
    "    def gradient_f(self, X, T):\n",
    "        Y = self.forward(X)\n",
    "        delta = -(T - Y) / (T.shape[0] * T.shape[1])\n",
    "        dVs = self.backpropagate(delta)\n",
    "        return self.pack(dVs)\n",
    "\n",
    "    def backpropagate(self, delta):\n",
    "        n_layers = len(self.Vs)\n",
    "        dVs = [np.zeros_like(V) for V in self.Vs]\n",
    "\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            V = self.Vs[layeri]\n",
    "            Z = np.insert(self.Zs[layeri - 1], 0, 1, axis=1) if layeri > 0 else np.insert(self.X, 0, 1, axis=1)\n",
    "\n",
    "            dVs[layeri] = np.dot(Z.T, delta)\n",
    "\n",
    "            if layeri > 0:\n",
    "                delta = (1 - self.Zs[layeri - 1] ** 2) * np.dot(delta, V[1:, :].T)\n",
    "\n",
    "        return dVs\n",
    "\n",
    "    def use(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def makeIndicatorVars(self, T):\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))\n",
    "        return (T == np.unique(T)).astype(int)\n",
    "\n",
    "    def softmax(self, Y):\n",
    "        maxY = np.max(Y, axis=1).reshape((-1, 1))\n",
    "        expY = np.exp(Y - maxY)\n",
    "        denom = np.sum(expY, axis=1).reshape((-1, 1))\n",
    "        return expY / denom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is a simple test of your new class.  For inputs from 0 to 100, classify values less than or equal to 25 as Class Label 25, greater than 25 and less than or equal to 75 as Class Label 75, and greater than 75 as Class Label 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:18:38.277218Z",
     "start_time": "2023-04-17T20:18:38.164683Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = np.arange(100).reshape((-1, 1))\n",
    "T = X.copy()\n",
    "T[T <= 25] = 25\n",
    "T[np.logical_and(25 < T, T <= 75)] = 75\n",
    "T[T > 75] = 100\n",
    "\n",
    "plt.plot(X, T, 'o-')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-17T20:19:45.625562Z",
     "start_time": "2023-04-17T20:19:16.097597Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hiddens = [10]\n",
    "nnet = NeuralNetworkClassifier(X.shape[1], hiddens, len(np.unique(T)))\n",
    "nnet.train(X, T, 200, 0.01, 'adam', True)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nnet.error_trace)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Likelihood')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(T + 5, 'o-', label='T + 5')  # to see, when predicted overlap T very closely\n",
    "plt.plot(nnet.use(X)[0], 'o-', label='Y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Now for the Hand-Drawn Digits\n",
    "\n",
    "We will use a bunch (50,000) images of hand drawn digits from [this deeplearning.net site](http://deeplearning.net/tutorial/gettingstarted.html).  Download `mnist.pkl.gz`. \n",
    "\n",
    "deeplearning.net goes down a lot.  If you can't download it from there you can try getting it from [here](https://gitlab.cs.washington.edu/colinxs/neural_nets/blob/master/mnist.pkl.gz).\n",
    "\n",
    "This pickle file includes data already partitioned into training, validation, and test sets.  To read it into python, use the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "Xtrain = train_set[0]\n",
    "Ttrain = train_set[1].reshape(-1, 1)\n",
    "\n",
    "Xval = valid_set[0]\n",
    "Tval = valid_set[1].reshape(-1, 1)\n",
    "\n",
    "Xtest = test_set[0]\n",
    "Ttest = test_set[1].reshape(-1, 1)\n",
    "\n",
    "print(Xtrain.shape, Ttrain.shape,  Xval.shape, Tval.shape,  Xtest.shape, Ttest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Ttrain[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Those must be the digits.  What the heck is in those 784 columns in the input matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(Xtrain[0, :]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Well, values between 0 and 1.  That doesn't help much.  These are actually intensity values for 784 pixels in an image.\n",
    "\n",
    "How can we rearrange these values into an image to be displayed?  We must first figure out how many columns and rows the image would have.  Perhaps the image is a square image, with equal numbers of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "math.sqrt(784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ah, cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok Let's reshape it and look at the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image0 = Xtrain[0, :]\n",
    "image0 = image0.reshape(28, 28)\n",
    "image0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Not that helpful.  Ok, let's use `matplotlib` to make an image display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(image0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Humm.  Try a grayscale color map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(image0, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With a little more work, we can make it look like a pencil drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(-image0, cmap='gray')  # notice the negative sign\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looks like a 5.  What class label is associated with this image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Ttrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Okay.  Makes sense.  Let's look at the first 100 images and their labels, as plot titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    plt.imshow(-Xtrain[i, :].reshape(28, 28), cmap='gray')\n",
    "    plt.title(Ttrain[i, 0])\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Okay.  We are ready to try to classify, right?\n",
    "\n",
    "First we should check the proportions of each digit in the given data partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = np.arange(10)\n",
    "(Ttrain == classes).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(Ttrain == classes).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(Ttrain == classes).sum(axis=0) / Ttrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "['Ttrain', *(Ttrain == classes).sum(axis=0) / Ttrain.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "result = []\n",
    "result.append(['Train', *(Ttrain == classes).sum(axis=0) / Ttrain.shape[0]])\n",
    "result.append(['Tval', *(Tval == classes).sum(axis=0) / Tval.shape[0]])\n",
    "result.append(['Ttest', *(Ttest == classes).sum(axis=0) / Ttest.shape[0]])\n",
    "pandas.DataFrame(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "All very close to 0.1. Super."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Time for our first experiment.  Let's train a small neural net with 5 hidden units in one layer for a small number of epochs using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "np.random.seed(142)\n",
    "\n",
    "nnet = NeuralNetworkClassifier(Xtrain.shape[1], [5], len(classes))\n",
    "nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(nnet)  # uses the __str__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(nnet.error_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now it is time for you to run some longer experiments.  You must write the code to do the following steps:\n",
    "\n",
    "1. For each of at least five different hidden layer structures\n",
    "\n",
    "    1. Train a network for 500 epochs.\n",
    "    1. Collect percent of samples correctly classified in the given train, validate, and test partitions.\n",
    "\n",
    "2. Create a `pandas.DataFrame` with these results and with column headings `('Hidden Layers', 'Train', 'Validate', 'Test', 'Time')` where `'Time'` is the number of seconds required to train each network.\n",
    "\n",
    "3. Retrain a network using the best hidden layer structure, judged by the percent correct on the validation set.\n",
    "4. Use this network to find several images in the test set for which the network's probability of the correct class is the closest to zero, meaning images for which your network does the worst.  Draw these images and discuss why your network might not be doing well for those images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## `confusion_matrix`\n",
    "\n",
    "Now, write a function named `confusion_matrix` that returns a confusion matrix for any classification problem, returned as a `pandas.DataFrame` as shown in Lecture Notes 12.  It must require two arguments, the predicted classes for each sample and the true classes for each sample.  Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def confusion_matrix(y_pred, y_true):\n",
    "    classes = np.unique(y_true)\n",
    "    num_classes = len(classes)\n",
    "    matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            matrix[i, j] = np.sum((y_true == classes[i]) & (y_pred == classes[j]))\n",
    "    return pd.DataFrame(matrix, columns=classes, index=classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Classes, Probs = nnet.use(Xtrain)\n",
    "table = []\n",
    "for true_class in range(1, 11):\n",
    "    row = []\n",
    "    for predicted_class in range(1, 11):\n",
    "        row.append(100 * np.mean(Classes[Ttrain == true_class] == predicted_class))\n",
    "        # row.append(f'{100 * np.mean(Classes[Ttrain == true_class] == predicted_class):0.1f}')\n",
    "    table.append(row)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_classes, Y_probs = nnet.use(Xtest)\n",
    "confusion_matrix(Y_classes, Ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Grading and Check-In\n",
    "\n",
    "You will receive 50 points for correct code, and 50 points for other results and your discussions.  As before, you can test your code against the grading script yourself by downloading [A4grader.zip](https://www.cs.colostate.edu/~cs445/notebooks/A4grader.zip) and extracting `A4grader.py` parallel to this notebook.  We recommend keeping this notebook and the grader script in a dedicated folder with *just those two files.* Run the code in the in the following cell to see an example grading run.  Submit assignments **through Canvas** following the pattern of the previous assignments. *Do not send your file to the instructor/TA via email or any other medium!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run -i A4grader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extra Credit\n",
    "Earn 5 extra credit point on this assignment by doing the following.\n",
    "\n",
    "1. Combine the train, validate, and test partitions loaded from the MNIST data file into two matrices, `X` and `T`. \n",
    "2. Using `adam` , `relu` and just one value of `learning_rate` and `n_epochs`, compare several hidden layer architectures. Do so by applying our `generate_k_fold_cross_validation_sets` function as defined in Lecture Notes 10 which forms stratified partitioning, for use in classification problems, to your `X` and `T` matrices using `n_fold` of 3.\n",
    "3. Show results and discuss which architectures you find works the best, and how you determined this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}